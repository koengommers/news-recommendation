architecture: BERT-NRMS
pretrained_model_name: bert-base-uncased
bert_pooling_method: attention
num_hidden_layers: 8
